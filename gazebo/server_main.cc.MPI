/*
 * Copyright (C) 2012 Open Source Robotics Foundation
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
*/
#include <memory>
#include <gazebo/ode/common.h>
#include "gazebo/common/Exception.hh"
#include "gazebo/util/LogRecord.hh"
#include "gazebo/common/Console.hh"
#include "gazebo/rendering/ogre_gazebo.h"
#include "gazebo/Server.hh"
#include "mpi.h"
#include "math.h"
#include <omp.h>
#include <iostream>
struct MPI_dxAABB{
  int il[2];
  double db[6];
};

struct MPI_dxAABB_Link{
  MPI_dxAABB* current;
  MPI_dxAABB* next;
};

struct MPI_Node{
  MPI_Node* next;
  int x,y,z;
  MPI_dxAABB* aabb;
};

unsigned long findVirtualAddress(int level,int x, int y, int z, int sz){
  return (1000*level + 100*x + 10*y + z) % sz;
}


void Build_MPI_Message_dxAABB(MPI_Datatype* MPI_Message_dxAABB){
  int block_lengths[2];
  block_lengths[0] = 2;
  block_lengths[1] = 6;

  MPI_Datatype typelist[2];
  typelist[0] = MPI_INT;
  typelist[1] = MPI_DOUBLE;

  MPI_Aint address[2];
  address[0] = 0;
  address[1] = 2 * sizeof(int);

  MPI_Type_struct(2, block_lengths, address, typelist, MPI_Message_dxAABB);
  MPI_Type_commit(MPI_Message_dxAABB);
}

//////////////////////////////////////////////////
int main(int argc, char **argv)
{
  int rank;
  int size;

  std::unique_ptr<gazebo::Server> server;

  MPI_Init(&argc, &argv);
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);
  char* proceeding = new char[2];
  //proceeding[0] =1;
  //proceeding[1] =1;
  if(rank==0){
    MPI_Status status_isend;
    MPI_Request handle_request;
    try{
      // Initialize the informational logger. This will log warnings, and errors
      gzLogInit("server-", "gzserver.log");
      // Initialize the data Logger. This will log state information.
      gazebo::util::LogRecord::Instance()->Init("gzserver");
      server.reset(new gazebo::Server());
      if(!server->ParseArgs(argc, argv))
        return -1;
      server->Run();
      server->Fini();

  //proceeding[0] = 0;
  //proceeding[1] = 0;
       std::cout << "serverfinirank=" << rank << std::endl;;
    //   MPI_Bcast(proceeding, 2, MPI_CHAR, 0, MPI_COMM_WORLD);
    int* n = new int[3];
    n[0]=0;
    n[1]=0;
    n[2]=0;
    
      MPI_Bcast(n, 3, MPI_INT, 0, MPI_COMM_WORLD); // recive the number of geoms n in current step
  MPI_Finalize();
	return 0;
  
    }catch(gazebo::common::Exception &_e){
      _e.Print();
      server->Fini();
      return -1;
    }catch(Ogre::Exception &_e){
      gzerr << "Ogre Error:" << _e.getFullDescription() << "\n";
      server->Fini();
      return -1;
    }
  }else if(rank!=0){
    MPI_Request handle_request;
    // prepare to handle collision detaction with mpi
    // Bcast list
    MPI_Datatype MPI_Message_dxAABB;
    Build_MPI_Message_dxAABB(&MPI_Message_dxAABB);
    int* n = new int[3];

    while(true){
   //   MPI_Bcast(proceeding, 2, MPI_CHAR, 0, MPI_COMM_WORLD);
      //if (proceeding[0]==0)break;
      MPI_Bcast(n, 3, MPI_INT, 0, MPI_COMM_WORLD); // recive the number of geoms n in current step
      if (n[2]==0)break;
      MPI_dxAABB* buffer = new MPI_dxAABB[n[0]];
      MPI_Bcast(buffer, n[0], MPI_Message_dxAABB, 0, MPI_COMM_WORLD); // recive the mpi aabb list
      //std::cout << "******************************" << std::endl;
      //for(int ii=0; ii<n[0]; ii++){
      //  std::cout << "rank=" << rank << " buffer[" << ii << "]=" << buffer[ii].il[0] << std::endl;;
      //}
      //std::cout << "******************************" << std::endl;
  
      int i=0;
      // for `n' objects, an n*n array of bits is used to record if those objects
      // have been intersection-tested against each other yet. this array can
      // grow large with high n, but oh well...
      int tested_rowsize = (n[0]+7) >> 3;      // number of bytes needed for n bits
      unsigned char *tested = (unsigned char *)malloc(n[0]*tested_rowsize*sizeof(unsigned char));
      memset (tested,0,n[0] * tested_rowsize);

      #define NUM_PRIMES 31
      const long int prime[NUM_PRIMES] = {1L,2L,3L,7L,13L,31L,61L,127L,251L,509L,
        1021L,2039L,4093L,8191L,16381L,32749L,65521L,131071L,262139L,
        524287L,1048573L,2097143L,4194301L,8388593L,16777213L,33554393L,
        67108859L,134217689L,268435399L,536870909L,1073741789L};
      // compute hash table size sz to be a prime > 8*n
      for (i=0; i<NUM_PRIMES; i++) {
        if (prime[i] >= (8*n[0])) break;
      }
      if (i >= NUM_PRIMES) i = NUM_PRIMES-1;        // probably pointless
      int sz = prime[i];
    
      // allocate and initialize hash table node pointers
      MPI_Node** table = new MPI_Node*[sz];
      for (i=0; i<sz; i++) table[i] = 0;
      // split virtual address to each process
      int block_num = size;
      int block_size = ceil(sz/size);
      int* range_of_address = new int[2];
      range_of_address[0] = (rank-1) * block_size;
      range_of_address[1] = (rank * block_size -1) < sz-1? (rank * block_size -1) : sz-1;
      // init table values
      //#pragma omp parallel for 
      for(i=0; i<n[0]; i++){
        for(int xi=buffer[i].db[0]; xi<=buffer[i].db[1]; xi++){
          for(int yi=buffer[i].db[2]; yi<=buffer[i].db[3]; yi++){
            for(int zi=buffer[i].db[4]; zi<=buffer[i].db[5]; zi++){
              unsigned long hi = findVirtualAddress(buffer[i].il[1], xi, yi, zi, sz);
              if(hi>=range_of_address[0] && hi<=range_of_address[1]){
                MPI_Node* node = new MPI_Node;
                node->x = xi;
                node->y = yi;
                node->z = zi;
                node->aabb = &buffer[i];
                node->next = table[hi];
                table[hi] =  node;
              }
            }
          }
        }
      }
  
      int* result = new int[2];
      result[0] = -1;
      result[1] = -1;
      int threads = n[0] / (size-1);
      // #pragma omp parallel for num_threads(threads)
      for(i=0; i<n[0]; i++){
        for(int level=buffer[i].il[1]; level<=n[1]; level++){
          for(int xi=buffer[i].db[0]; xi<=buffer[i].db[1]; xi++){
            for(int yi=buffer[i].db[2]; yi<=buffer[i].db[3]; yi++){
              for(int zi=buffer[i].db[4]; zi<=buffer[i].db[5]; zi++){
                unsigned long hi = findVirtualAddress(level, xi, yi, zi, sz);
                //std::cout << "xi=" << xi << " yi=" << yi << " zi=" << zi << " level=" << std::endl;
                if(hi>=range_of_address[0] && hi<=range_of_address[1]){
                  MPI_Node* node;
                  //std::cout << "rank=" << rank << " buffer[" << i << "] index:" << buffer[i].il[1] << " n[0]=" << n[0] << std::endl; 
                  for (node = table[hi]; node; node=node->next) {
                    //std::cout << " get node " << std::endl;
                    // node points to an AABB that may intersect aabb
                    if (node->aabb->il[0] == buffer[i].il[0]) 
                      continue;
                    if (node->aabb->il[1] == level &&
                        node->x == xi && node->y == yi && node->z == zi) {
                      // see if aabb and node->aabb have already been tested against each other
                      unsigned char mask;
                      int iii;
                      if (buffer[i].il[0] <= node->aabb->il[0]) {
                        iii = (buffer[i].il[0] * tested_rowsize)+(node->aabb->il[0] >> 3);
                        mask = 1 << (node->aabb->il[0] & 7);
                      }else{
                        iii = (node->aabb->il[0] * tested_rowsize)+(buffer[i].il[0] >> 3);
                        mask = 1 << (buffer[n[0]].il[0] & 7);
                      }
                      dIASSERT (iii >= 0 && iii < (tested_rowsize*n[0]));
                      if ((tested[iii] & mask)==0) {
                        result[0] = buffer[i].il[0];
                        result[1] = node->aabb->il[0];
                        //std::cout  <<"rank:" << rank << ":" << result[0] << ":" << result[1] << std::endl;
                        MPI_Send(result, 2, MPI_INT, 0, rank, MPI_COMM_WORLD);
                        //std::cout  <<"rank:" << rank << "test send result end" << std::endl;
                      }
                      tested[i] |= mask;
                    }
                  }
                }
              }
            }
          }
        }
        //std::cout << "()()()()" << std::endl;
      }
      //std::cout  <<"rank:" << rank << "Test!!!! result=" << result[0] << std::endl;
      MPI_Send(result, 2, MPI_INT, 0, 10, MPI_COMM_WORLD);
      //MPI_Irecv(proceeding,2,MPI_INT,0,99,MPI_COMM_WORLD,&handle_request);
      //std::cout  <<"rank:" << rank << "!!!!Test"<<std::endl;
    }
    //std::cout << "last" << std::endl;
  } 
  MPI_Finalize();

       std::cout << "mpifinirank=" << rank << std::endl;;
  return 0;
}
